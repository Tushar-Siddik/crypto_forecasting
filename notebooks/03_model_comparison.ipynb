{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison for Cryptocurrency Forecasting\n",
    "\n",
    "This notebook compares different deep learning models for cryptocurrency price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our modules\n",
    "from data.data_loader import CryptoDataLoader\n",
    "from data.feature_engineering import FeatureEngineer\n",
    "from data.preprocessor import DataPreprocessor\n",
    "from models.lstm_attention import LSTMAttention, GRUAttention\n",
    "from models.transformer import TransformerModel, InformerModel\n",
    "from models.ensemble import EnsembleModel\n",
    "from training.trainer import ModelTrainer\n",
    "from evaluation.metrics import CryptoModelEvaluator\n",
    "from evaluation.visualizer import ModelVisualizer\n",
    "from utils.helpers import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'ticker': 'BTC-USD',\n",
    "    'sequence_length': 30,\n",
    "    'prediction_horizon': 1,\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,  # Reduced for notebook demonstration\n",
    "    'lr': 0.001,\n",
    "    'patience': 5\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "print(\"Loading and preparing data...\")\n",
    "\n",
    "# Initialize components\n",
    "loader = CryptoDataLoader('../data/raw')\n",
    "feature_engineer = FeatureEngineer()\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Load data\n",
    "data = loader.get_latest_data(config['ticker'], days=365)\n",
    "print(f\"Loaded {len(data)} records for {config['ticker']}\")\n",
    "\n",
    "# Feature engineering\n",
    "data_with_features = feature_engineer.add_technical_indicators(data)\n",
    "print(f\"Added {len(feature_engineer.feature_columns)} features\")\n",
    "\n",
    "# Select top features\n",
    "selected_features = feature_engineer.select_features(\n",
    "    data_with_features, \n",
    "    method='correlation', \n",
    "    top_k=20\n",
    ")\n",
    "print(f\"Selected {len(selected_features)} features\")\n",
    "\n",
    "# Create sequences\n",
    "X, y = feature_engineer.create_sequences(\n",
    "    data_with_features, \n",
    "    config['sequence_length'],\n",
    "    config['prediction_horizon']\n",
    ")\n",
    "print(f\"Created sequences: X shape={X.shape}, y shape={y.shape}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocessor.split_data(\n",
    "    X, y, \n",
    "    config['train_ratio'], \n",
    "    config['val_ratio']\n",
    ")\n",
    "print(f\"Data split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "# Scale data\n",
    "X_train_scaled, X_val_scaled, X_test_scaled, y_train_scaled, y_val_scaled, y_test_scaled = preprocessor.scale_data(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test\n",
    ")\n",
    "print(\"Data scaled successfully\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled), \n",
    "    torch.FloatTensor(y_train_scaled)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val_scaled), \n",
    "    torch.FloatTensor(y_val_scaled)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_scaled), \n",
    "    torch.FloatTensor(y_test_scaled)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "print(\"Data loaders created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare\n",
    "input_size = len(selected_features)\n",
    "models = {\n",
    "    'LSTM': LSTMAttention(\n",
    "        input_size=input_size,\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        output_size=1,\n",
    "        dropout=0.2,\n",
    "        bidirectional=True,\n",
    "        attention_heads=4\n",
    "    ),\n",
    "    'GRU': GRUAttention(\n",
    "        input_size=input_size,\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        output_size=1,\n",
    "        dropout=0.2,\n",
    "        bidirectional=True,\n",
    "        attention_heads=4\n",
    "    ),\n",
    "    'Transformer': TransformerModel(\n",
    "        input_size=input_size,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        dim_feedforward=128,\n",
    "        output_size=1,\n",
    "        dropout=0.1\n",
    "    ),\n",
    "    'Informer': InformerModel(\n",
    "        input_size=input_size,\n",
    "        d_model=64,\n",
    "        nhead=4,\n",
    "        num_encoder_layers=2,\n",
    "        num_decoder_layers=1,\n",
    "        dim_feedforward=128,\n",
    "        output_size=1,\n",
    "        dropout=0.1\n",
    "    )\n",
    "}\n",
    "\n",
    "# Display model information\n",
    "print(\"Model Information:\")\n",
    "for name, model in models.items():\n",
    "    info = model.get_model_info()\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Total parameters: {info['total_parameters']:,}\")\n",
    "    print(f\"  Trainable parameters: {info['trainable_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "model_results = {}\n",
    "model_predictions = {}\n",
    "model_histories = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name} Model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = ModelTrainer(model, device)\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        lr=config['lr'],\n",
    "        patience=config['patience'],\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics, predictions, actuals = trainer.evaluate(test_loader, preprocessor.target_scaler)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = metrics\n",
    "    model_predictions[name] = predictions\n",
    "    model_histories[name] = history\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "print(\"Model Comparison Results:\")\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "display(results_df)\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "for metric in results_df.columns:\n",
    "    if metric in ['MAE', 'MSE', 'RMSE', 'MAPE', 'MASE', 'Max Drawdown', 'Error Volatility']:\n",
    "        best_model = results_df[metric].idxmin()\n",
    "    else:\n",
    "        best_model = results_df[metric].idxmax()\n",
    "    print(f\"  {metric}: {best_model} ({results_df[metric][best_model]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "visualizer = ModelVisualizer()\n",
    "visualizer.plot_model_comparison(\n",
    "    model_results, \n",
    "    metrics=['MAE', 'RMSE', 'MAPE', 'Directional Accuracy', 'Sharpe Ratio'],\n",
    "    save_path='../results/model_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for all models\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "n_points = min(100, len(actuals))\n",
    "x_axis = range(n_points)\n",
    "plt.plot(x_axis, actuals[:n_points], label='Actual', color='black', linewidth=3)\n",
    "\n",
    "# Plot predictions for each model\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "for i, (name, predictions) in enumerate(model_predictions.items()):\n",
    "    plt.plot(x_axis, predictions[:n_points], label=name, \n",
    "             color=colors[i % len(colors)], linewidth=2, linestyle='--')\n",
    "\n",
    "plt.title('Model Predictions Comparison', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction errors\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, predictions) in enumerate(model_predictions.items()):\n",
    "    if i < len(axes):\n",
    "        errors = actuals - predictions\n",
    "        axes[i].hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'{name} Prediction Errors')\n",
    "        axes[i].set_xlabel('Error')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add vertical line at zero\n",
    "        axes[i].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training History Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot training loss\n",
    "for name, history in model_histories.items():\n",
    "    axes[0].plot(history['train_loss'], label=f'{name} Train')\n",
    "    axes[1].plot(history['val_loss'], label=f'{name} Val')\n",
    "\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_title('Validation Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate\n",
    "for name, history in model_histories.items():\n",
    "    if 'learning_rate' in history:\n",
    "        axes[2].plot(history['learning_rate'], label=name)\n",
    "\n",
    "axes[2].set_title('Learning Rate')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Learning Rate')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot epoch time\n",
    "for name, history in model_histories.items():\n",
    "    if 'epoch_time' in history:\n",
    "        axes[3].plot(history['epoch_time'], label=name)\n",
    "\n",
    "axes[3].set_title('Epoch Time')\n",
    "axes[3].set_xlabel('Epoch')\n",
    "axes[3].set_ylabel('Time (seconds)')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble model\n",
    "base_models = list(models.values())\n",
    "ensemble = EnsembleModel(\n",
    "    models=base_models,\n",
    "    input_size=input_size,\n",
    "    output_size=1,\n",
    "    aggregation_method='weighted_average'\n",
    ")\n",
    "\n",
    "print(\"Ensemble Model Information:\")\n",
    "ensemble_info = ensemble.get_model_info()\n",
    "for key, value in ensemble_info.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Train ensemble\n",
    "print(\"\\nTraining Ensemble Model...\")\n",
    "ensemble_trainer = ModelTrainer(ensemble, device)\n",
    "\n",
    "ensemble_history = ensemble_trainer.train(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=config['epochs'],\n",
    "    lr=config['lr'],\n",
    "    patience=config['patience'],\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_metrics, ensemble_predictions, ensemble_actuals = ensemble_trainer.evaluate(\n",
    "    test_loader, preprocessor.target_scaler\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble Results:\")\n",
    "for metric, value in ensemble_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# Add ensemble to results\n",
    "model_results['Ensemble'] = ensemble_metrics\n",
    "model_predictions['Ensemble'] = ensemble_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Comparison with Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated comparison with ensemble\n",
    "final_results_df = pd.DataFrame(model_results).T\n",
    "print(\"Final Model Comparison (with Ensemble):\")\n",
    "display(final_results_df)\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest Models by Metric (including Ensemble):\")\n",
    "for metric in final_results_df.columns:\n",
    "    if metric in ['MAE', 'MSE', 'RMSE', 'MAPE', 'MASE', 'Max Drawdown', 'Error Volatility']:\n",
    "        best_model = final_results_df[metric].idxmin()\n",
    "    else:\n",
    "        best_model = final_results_df[metric].idxmax()\n",
    "    print(f\"  {metric}: {best_model} ({final_results_df[metric][best_model]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final comparison\n",
    "visualizer.plot_model_comparison(\n",
    "    model_results, \n",
    "    metrics=['MAE', 'RMSE', 'MAPE', 'Directional Accuracy', 'Sharpe Ratio'],\n",
    "    save_path='../results/final_model_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ensemble vs best individual model\n",
    "best_individual_model = final_results_df['RMSE'].idxmin()\n",
    "print(f\"Comparing Ensemble with best individual model: {best_individual_model}\")\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Plot actual values\n",
    "n_points = min(100, len(ensemble_actuals))\n",
    "x_axis = range(n_points)\n",
    "plt.plot(x_axis, ensemble_actuals[:n_points], label='Actual', color='black', linewidth=3)\n",
    "\n",
    "# Plot best individual model\n",
    "plt.plot(x_axis, model_predictions[best_individual_model][:n_points], \n",
    "         label=best_individual_model, color='blue', linewidth=2, linestyle='--')\n",
    "\n",
    "# Plot ensemble\n",
    "plt.plot(x_axis, ensemble_predictions[:n_points], \n",
    "         label='Ensemble', color='red', linewidth=2, linestyle='-')\n",
    "\n",
    "plt.title('Ensemble vs Best Individual Model', fontsize=16)\n",
    "plt.xlabel('Time', fontsize=12)\n",
    "plt.ylabel('Price (USD)', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "summary_stats = pd.DataFrame()\n",
    "\n",
    "for name, metrics in model_results.items():\n",
    "    summary_stats[name] = pd.Series({\n",
    "        'RMSE': metrics['RMSE'],\n",
    "        'MAE': metrics['MAE'],\n",
    "        'MAPE': metrics['MAPE'],\n",
    "        'Directional Accuracy': metrics['Directional Accuracy'],\n",
    "        'Sharpe Ratio': metrics['Sharpe Ratio'],\n",
    "        'Training Time (s)': sum(model_histories[name]['epoch_time']) if name in model_histories else 0\n",
    "    })\n",
    "\n",
    "# Rank models\n",
    "summary_stats['RMSE_Rank'] = summary_stats['RMSE'].rank()\n",
    "summary_stats['MAE_Rank'] = summary_stats['MAE'].rank()\n",
    "summary_stats['Directional_Accuracy_Rank'] = summary_stats['Directional Accuracy'].rank(ascending=False)\n",
    "summary_stats['Average_Rank'] = summary_stats[['RMSE_Rank', 'MAE_Rank', 'Directional_Accuracy_Rank']].mean(axis=1)\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "display(summary_stats.sort_values('Average_Rank'))\n",
    "\n",
    "# Save results\n",
    "summary_stats.to_csv('../results/model_performance_summary.csv')\n",
    "print(\"\\nResults saved to '../results/model_performance_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Best Performing Model**: [Based on results]\n",
    "2. **Ensemble Performance**: The ensemble model typically provides more stable predictions\n",
    "3. **Training Speed**: [Fastest model] trains quickest, while [slowest model] takes longest\n",
    "4. **Prediction Accuracy**: All models achieve reasonable accuracy with [best metric] being [value]\n",
    "5. **Directional Accuracy**: [Best model] is most accurate at predicting price direction\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Production**: Use the ensemble model for more robust predictions\n",
    "2. **For Speed**: Use [fastest accurate model] for real-time applications\n",
    "3. **For Research**: Experiment with different architectures and hyperparameters\n",
    "4. **For Improvement**: Consider adding more features or using longer sequences\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Use automated tuning for each model\n",
    "2. **Feature Selection**: Experiment with different feature selection methods\n",
    "3. **Multi-step Prediction**: Extend to predict multiple days ahead\n",
    "4. **Probabilistic Forecasting**: Add prediction intervals\n",
    "5. **Online Learning**: Implement continuous learning for market adaptation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}