{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3502fe7a",
   "metadata": {},
   "source": [
    "setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd7a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure src folder is importable\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Auto-reload changes in .py files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962eed1",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405701af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.evaluation.metrics import evaluate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832ec1e",
   "metadata": {},
   "source": [
    "Load or define predictions and actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d3e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: simulated predictions and actual values\n",
    "# Replace these with outputs from your model\n",
    "np.random.seed(42)\n",
    "n_points = 100\n",
    "y_true = np.cumsum(np.random.randn(n_points)) + 100  # Simulated actual prices\n",
    "y_pred = y_true + np.random.randn(n_points) * 2       # Simulated model predictions\n",
    "\n",
    "# Display first 5 values\n",
    "print(\"Actual:\", y_true[:5])\n",
    "print(\"Predicted:\", y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198513f",
   "metadata": {},
   "source": [
    "Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = evaluate_model(y_true, y_pred)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332c913",
   "metadata": {},
   "source": [
    "Plot actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_true, label='Actual', linewidth=2)\n",
    "plt.plot(y_pred, label='Predicted', linewidth=2)\n",
    "plt.title(\"Actual vs Predicted Values\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadb16f5",
   "metadata": {},
   "source": [
    "Plot directional accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate up/down direction\n",
    "direction_true = np.diff(y_true) > 0\n",
    "direction_pred = np.diff(y_pred) > 0\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(direction_true.astype(int), label='Actual Direction', marker='o')\n",
    "plt.plot(direction_pred.astype(int), label='Predicted Direction', marker='x')\n",
    "plt.title(\"Directional Accuracy\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Direction (0=Down, 1=Up)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print directional accuracy\n",
    "directional_accuracy = np.mean(direction_true == direction_pred)\n",
    "print(f\"Directional Accuracy: {directional_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
